#This file is to get a hold of the format of the responses generated by ChatOpenAI after you
#send a prompt. This will help us implement key features in the backend, including necessary
#string manipulation and list manipulations.

from langchain.llms import openai
import os
os.environ["OPENAI_API_KEY"] = "<Your API Key"
openai.api_key = os.getenv("OPENAI_API_KEY")

from langchain import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
import streamlit as st

template = """
You are an expert quiz maker for any major quiz contexts, especially technical fields. Let's think step by step and
create a quiz with {num_questions} multiple-choice questions about the following concept/content: {quiz_context}.

The format of the quiz could be one of the following:
- Multiple-choice: 
- Questions:
    <Question1>: <a. Answer 1>, <b. Answer 2>, <c. Answer 3>, <d. Answer 4>
    <Question2>: <a. Answer 1>, <b. Answer 2>, <c. Answer 3>, <d. Answer 4>
    ....
- Answers:
    <Answer1>: <a|b|c|d>
    <Answer2>: <a|b|c|d>
    ....
    Example:
    - Questions:
    - 1. What is the time complexity of a binary search tree?
        a. O(n)
        b. O(log n)
        c. O(n^2)
        d. O(1)
    - Answers: 
        1. b
"""

prompt = PromptTemplate.from_template(template)
prompt.format(num_questions=3, quiz_context="Data Structures in Python Programming")
chain = LLMChain(llm=ChatOpenAI(temperature=0.0),
                 prompt=prompt)
quiz_response = chain.run(num_questions=3, quiz_context="Major cities and their capitals",)
print(quiz_response)